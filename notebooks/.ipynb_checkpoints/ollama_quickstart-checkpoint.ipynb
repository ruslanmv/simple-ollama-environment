{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Ollama Quickstart\n",
    "This notebook demonstrates a minimal inference flow using the **Ollama** Python client.\n",
    "\n",
    "It expects an Ollama server reachable at `http://localhost:11434`.\n",
    "\n",
    "**Tip:** In the Docker image included with this project, the Ollama server starts automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Python client `ollama` is available.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "try:\n",
    "    import ollama\n",
    "    print('‚úÖ Python client `ollama` is available.')\n",
    "except Exception as e:\n",
    "    print('‚ùå Missing python package `ollama`. Install with `pip install ollama`.')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull a tiny model (first time only)\n",
    "We use **Qwen2.5 0.5B Instruct** ‚Äî a compact model that works well on 8 GB RAM setups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Model ready: qwen2.5:0.5b-instruct\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ollama.pull('qwen2.5:0.5b-instruct')\n",
    "    print('üì• Model ready: qwen2.5:0.5b-instruct')\n",
    "except Exception as e:\n",
    "    print('‚ö†Ô∏è Could not pull the model. Is the Ollama server running on http://localhost:11434 ?')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a chat inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Salutami!\" in italiano √® \"Ciao!\". Quindi, se vuoi chiamare me con l'accento sul \"s\", puoi dire \"Sallo' ciao!\" per \"Ciao!\". In generale, la forma \"salutami\" o \"salutamelo\" √® pi√π comune e utilizzata.\n",
      "\n",
      "Per studiare meglio per il italiano, non esitare a cercare aiuto. Sperimenta con le formule del testo originale e con l'uso delle frasi come \"Ciao!\", \"Salute\", o \"Grazie!\" se hai notato problemi o vuoi migliorare il tuo modo di parlare.\n",
      "\n",
      "Oggi, nonostante sia difficile, si pu√≤ praticamente parlare italiano. In generale, la maggior parte degli iniziative semplificate come l'uso dei gesti (es. \"Salutami!\"), l'apprendimento delle parole come \"ciao\", \"salute\" e \"grazie!\" sono comuni.\n",
      "\n",
      "Ricorda di fare i conti con la tua abilit√† linguistica, soprattutto per le frasi che non sei completamente convinto di parlare. Se stai cercando un modo pi√π comune per comunicarti in italiano o se hai bisogno di migliorare il tuo modo di parlare, ti consiglio di fare l'analisi dei tuoi passi e fare l'esperienza con i concetti e le frasi.\n"
     ]
    }
   ],
   "source": [
    "resp = ollama.chat(\n",
    "    model='qwen2.5:0.5b-instruct',\n",
    "    messages=[{'role':'user','content':'Di\\' solo \"Ciao!\" in italiano e poi dammi 1 consiglio per studiare meglio.'}],\n",
    ")\n",
    "print(resp['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change model (optional)\n",
    "If you've pulled a different small model (e.g., `llama3.2:1b`), change the `model=` argument above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
